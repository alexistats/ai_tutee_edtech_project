# AI Tutee - System Architecture Description

> This document describes the system architecture in a structured format suitable for LLM-based diagram generation tools.

## Overview

**AI Tutee** is a "learning by teaching" educational web application where users practice teaching data visualization concepts to an AI student powered by OpenAI's language models.

---

## High-Level System Architecture (Mermaid)

```mermaid
flowchart TD
    subgraph Setup["1. Setup"]
        A[/"ğŸ‘¤ Teacher"/] --> B["Select Scenario"]
        B --> C["Select Knowledge Level<br/>(Beginner / Intermediate / Advanced)"]
    end

    subgraph PreTest["2. Pre-Instruction Assessment"]
        C --> D["ğŸ¤– AI Student<br/>Takes MCQ Pre-Test"]
        D --> E["Assessment Module<br/>Grades Responses"]
        E --> F["Display Results<br/>(Question Cards)"]
    end

    subgraph Teaching["3. Teaching Interaction"]
        F --> G["Teacher Selects<br/>Question to Teach"]
        G --> H["ğŸ¤– AI Student<br/>Responds with Misconceptions"]
        H --> I["ğŸ‘¤ Teacher<br/>Provides Instruction"]
        I --> H
        H --> J{"Done Teaching<br/>This Question?"}
        J -- "No" --> I
        J -- "Yes" --> K["Summarize Learning"]
        K --> L{"More Questions<br/>to Teach?"}
        L -- "Yes" --> G
        L -- "No" --> M["End Teaching Session"]
    end

    subgraph PostTest["4. Post-Instruction Assessment"]
        M --> N["ğŸ¤– AI Student<br/>Takes MCQ Post-Test<br/>(with Learning Context)"]
        N --> O["Assessment Module<br/>Grades & Compares"]
        O --> P["ğŸ“Š Display Results<br/>Pre vs Post Improvement"]
    end

    subgraph External["External Services"]
        API[("OpenAI API<br/>GPT-4 / GPT-4o-mini")]
    end

    D <-..-> API
    H <-..-> API
    K <-..-> API
    N <-..-> API

    style Setup fill:#e1f5fe
    style PreTest fill:#fff3e0
    style Teaching fill:#e8f5e9
    style PostTest fill:#fce4ec
    style External fill:#f3e5f5
```

### Simplified Flow Diagram

```mermaid
flowchart LR
    A["ğŸ¯ Scenario +<br/>Level Selection"] --> B["ğŸ“ Pre-Test<br/>(AI Student)"]
    B --> C["ğŸ’¬ Teaching<br/>Interaction"]
    C --> C
    C --> D["ğŸ“Š Post-Test<br/>Results"]

    style A fill:#e3f2fd,stroke:#1976d2
    style B fill:#fff8e1,stroke:#f57c00
    style C fill:#e8f5e9,stroke:#388e3c
    style D fill:#fce4ec,stroke:#c2185b
```

### User Interface & Instructional Flow Diagram

```mermaid
flowchart TD
    subgraph Welcome["Welcome Screen"]
        W1["ğŸ“– View Instructions<br/>& Learning Objectives"]
    end

    subgraph Setup["Setup Phase"]
        S1["Select Teaching Scenario"]
        S2["Choose Knowledge Level"]
        S3["Configure Advanced Settings<br/>(Optional: Policy, Tone, Turn Budget)"]
        S4["Click 'Start Session'"]
        S1 --> S2 --> S3 --> S4
    end

    subgraph PreTest["Pre-Test Phase"]
        PT1["ğŸ¤– AI Student<br/>Answers 5 MCQs"]
        PT2["System Grades<br/>Responses"]
        PT3["Display Question Cards<br/>with Status Indicators"]
        PT1 --> PT2 --> PT3
    end

    subgraph QuestionReview["Question Selection"]
        QR1["Review Question Cards:<br/>âœ“ Correct | âœ— Incorrect"]
        QR2{"Select Question<br/>to Teach"}
    end

    subgraph Teaching["Teaching Interaction"]
        T1["View Question Details<br/>(Question, AI's Answer, Correct Answer)"]
        T2["ğŸ¤– AI Student Opens<br/>with Misconception"]
        T3["ğŸ‘¤ Teacher Types<br/>Instruction"]
        T4["ğŸ¤– AI Student<br/>Responds"]
        T5{"Teacher Satisfied?"}
        T6["Click 'Done with<br/>This Question'"]
        T7["ğŸ“ System Summarizes<br/>Learning Outcome"]

        T1 --> T2
        T2 --> T3
        T3 --> T4
        T4 --> T5
        T5 -- "No" --> T3
        T5 -- "Yes" --> T6
        T6 --> T7
    end

    subgraph PostTest["Post-Test Phase"]
        PO1["ğŸ¤– AI Student Retakes MCQs<br/>(with Learning Context)"]
        PO2["System Grades &<br/>Compares to Pre-Test"]
    end

    subgraph Results["Results Screen"]
        R1["ğŸ“Š View Score Comparison<br/>(Pre vs Post)"]
        R2["ğŸ“‹ Review Learning<br/>Summaries per Question"]
        R3["Analyze Improvement<br/>Metrics"]
    end

    W1 --> S1
    S4 --> PT1
    PT3 --> QR1
    QR1 --> QR2
    QR2 --> T1
    T7 --> QR3{"Teach More<br/>Questions?"}
    QR3 -- "Yes" --> QR2
    QR3 -- "No, End Session" --> PO1
    PO2 --> R1
    R1 --> R2 --> R3

    R3 --> Reset{"Start New<br/>Session?"}
    Reset -- "Yes" --> S1

    style Welcome fill:#e8eaf6,stroke:#3f51b5
    style Setup fill:#e3f2fd,stroke:#1976d2
    style PreTest fill:#fff8e1,stroke:#ff8f00
    style QuestionReview fill:#fff3e0,stroke:#ef6c00
    style Teaching fill:#e8f5e9,stroke:#388e3c
    style PostTest fill:#fce4ec,stroke:#c2185b
    style Results fill:#f3e5f5,stroke:#7b1fa2
```

### System Architecture Diagram (Figure 1)

```mermaid
flowchart TB
    subgraph Frontend["4.2.1 Frontend & Session Management"]
        direction TB

        subgraph UI["Streamlit Web Application"]
            UI_Welcome["Welcome Screen"]
            UI_Setup["Setup Sidebar"]
            UI_PreTest["Pre-Test Review"]
            UI_Teaching["Teaching Interface"]
            UI_Results["Results Dashboard"]
        end

        subgraph SessionPhases["Session Phases"]
            direction LR
            P1["setup"] --> P2["pre-test"]
            P2 --> P3["teaching"]
            P3 --> P4["post-test"]
            P4 --> P5["results"]
        end

        subgraph SessionState["Session State (In-Memory)"]
            SS_Messages["Messages"]
            SS_Scores["Scores<br/>(Pre/Post)"]
            SS_Summary["Learning<br/>Summaries"]
            SS_Question["Selected<br/>Question"]
        end
    end

    subgraph Backend["4.2.2 Backend Service Layer"]
        direction TB

        subgraph Services["Core Services"]
            SVC_Prompt["Prompt Loader<br/><i>prompt_loader.py</i>"]
            SVC_Assess["Assessment Engine<br/><i>assessment.py</i>"]
            SVC_Scenario["Scenario Engine<br/><i>YAML configs</i>"]
            SVC_Log["Logging Layer<br/><i>io.py</i>"]
        end

        subgraph Data["Data Stores"]
            DATA_Scenarios[("Scenario Files<br/>/app/scenarios/*.yaml")]
            DATA_Prompts[("Prompt Templates<br/>/app/prompts/*.md")]
            DATA_Logs[("Session Logs<br/>/logs/runs/")]
        end
    end

    subgraph External["4.2.3 External LLM Integration"]
        direction TB

        subgraph OpenAI["OpenAI API"]
            API_Endpoint["Chat Completions<br/>Endpoint"]
            API_Models["Models:<br/>GPT-4 | GPT-4o-mini"]
            API_Settings["Settings:<br/>temp=0.7 (sampling)<br/>temp=0.1 (grading)"]
        end
    end

    %% Connections: Frontend to Backend
    UI --> SessionState
    SessionState --> Services
    SessionPhases -.-> UI

    %% Connections: Backend internal
    SVC_Scenario --> DATA_Scenarios
    SVC_Prompt --> DATA_Prompts
    SVC_Log --> DATA_Logs
    SVC_Prompt --> SVC_Assess
    SVC_Scenario --> SVC_Assess

    %% Connections: Backend to External
    SVC_Assess <-->|"API Calls"| API_Endpoint
    SVC_Prompt -->|"System Prompt"| API_Endpoint

    %% Styling
    style Frontend fill:#e3f2fd,stroke:#1565c0
    style Backend fill:#e8f5e9,stroke:#2e7d32
    style External fill:#fff3e0,stroke:#ef6c00
    style UI fill:#bbdefb
    style SessionState fill:#b3e5fc
    style SessionPhases fill:#b3e5fc
    style Services fill:#c8e6c9
    style Data fill:#a5d6a7
    style OpenAI fill:#ffe0b2
```

### Component Diagram (Figure 5)

```mermaid
flowchart LR
    subgraph Orchestrator["main_streamlit.py<br/>(Orchestrator)"]
        ORCH["Session Controller"]
    end

    subgraph Utilities["Utility Modules"]
        PROMPT["prompt_loader.py<br/>Template Engine"]
        ASSESS["assessment.py<br/>Test & Grading"]
        IO["io.py<br/>File Operations"]
    end

    subgraph Configs["Configuration Files"]
        YAML1["data_types.yaml"]
        YAML2["type_to_chart.yaml"]
        YAML3["chart_to_task.yaml"]
        YAML4["data_preparation.yaml"]
        MD["system_ai_student.md"]
    end

    subgraph External["External"]
        API[("OpenAI API")]
    end

    subgraph Output["Output"]
        JSONL[("*.jsonl<br/>Transcripts")]
        JSON[("*_summary.json<br/>Results")]
    end

    %% Dependencies
    ORCH --> PROMPT
    ORCH --> ASSESS
    ORCH --> IO

    MD --> PROMPT
    YAML1 --> ASSESS
    YAML2 --> ASSESS
    YAML3 --> ASSESS
    YAML4 --> ASSESS

    ORCH <--> API
    ASSESS <--> API

    IO --> JSONL
    IO --> JSON

    style Orchestrator fill:#e1bee7,stroke:#7b1fa2
    style Utilities fill:#c8e6c9,stroke:#388e3c
    style Configs fill:#fff9c4,stroke:#f9a825
    style External fill:#ffe0b2,stroke:#ef6c00
    style Output fill:#b2dfdb,stroke:#00695c
```

---

## Architecture Diagram Specification

### System Components

```
COMPONENTS:
1. User Interface Layer (Streamlit Web App)
2. Session State Manager
3. Scenario Configuration Engine
4. Assessment Module
5. AI Student Engine (LLM Integration)
6. Prompt System
7. Logging & Analytics
8. External Services (OpenAI API)
```

### Component Details

#### 1. User Interface Layer
- **Technology**: Streamlit (Python)
- **Screens**: Welcome, Setup Sidebar, Pre-Test Review, Teaching Interface, Results
- **Interactions**: User input forms, chat interface, question cards, score displays

#### 2. Session State Manager
- **Storage**: In-memory (Streamlit session_state)
- **Tracks**: Session phase, messages, scores, selected questions, learning summaries
- **Phases**: setup â†’ pre_test_review â†’ teaching â†’ results

#### 3. Scenario Configuration Engine
- **Source**: YAML files in `/app/scenarios/`
- **Scenarios**:
  - data_types (Data type identification)
  - type_to_chart (Chart type mapping)
  - chart_to_task (Task-driven selection)
  - data_preparation (Data cleaning)
- **Per-Level Config**: misconceptions, subskills, vocabulary_comfort, question_style, turn_budget

#### 4. Assessment Module
- **Location**: `/app/util/assessment.py`
- **Functions**: Pre-test, Post-test, MCQ grading, Learning summarization
- **MCQ Database**: 5 questions Ã— 3 levels Ã— 4 scenarios = 60 total questions

#### 5. AI Student Engine
- **Model**: OpenAI GPT-4/GPT-4o-mini (configurable)
- **Behavior**: Holds misconceptions as genuine beliefs, level-appropriate responses
- **Policies**: withhold_solution, guided_steps, full_solution_ok

#### 6. Prompt System
- **Template**: `/app/prompts/system_ai_student.md`
- **Dynamic Variables**: misconceptions, subskills, knowledge_level, tone, turn_budget
- **Loader**: `/app/util/prompt_loader.py`

#### 7. Logging & Analytics
- **Output Directory**: `/logs/runs/`
- **Transcript Format**: JSONL (per-message records with metadata)
- **Summary Format**: JSON (session scores, questions worked on)

#### 8. External Services
- **OpenAI API**: Chat Completions endpoint
- **Authentication**: API key via environment variable

---

## Data Flow Diagram

```
DATA FLOW (Sequential):

[User]
  â†“ selects scenario + knowledge level
[Setup Phase]
  â†“ loads YAML config
[Scenario Engine]
  â†“ builds system prompt
[Prompt System]
  â†“ sends to OpenAI with MCQs
[AI Student Engine] â†â†’ [OpenAI API]
  â†“ returns pre-test answers
[Assessment Module]
  â†“ grades responses, shows question cards
[Pre-Test Review UI]
  â†“ teacher selects question to teach
[Teaching Interface]
  â†“ conversation loop
[AI Student Engine] â†â†’ [OpenAI API]
  â†“ teacher marks complete
[Learning Summarization]
  â†“ extracts what was taught
[Post-Test Phase]
  â†“ applies learning context
[Assessment Module] â†â†’ [OpenAI API]
  â†“ grades and compares
[Results UI]
  â†“ displays improvement
[Logging System]
  â†“ writes JSONL + JSON
[File System]
```

---

## Component Relationship Diagram

```
RELATIONSHIPS:

User Interface Layer
  â”œâ”€â”€ USES â†’ Session State Manager
  â”œâ”€â”€ DISPLAYS â†’ Assessment Module results
  â”œâ”€â”€ SENDS INPUT TO â†’ AI Student Engine
  â””â”€â”€ SHOWS â†’ Logging & Analytics summaries

Session State Manager
  â”œâ”€â”€ STORES â†’ conversation messages
  â”œâ”€â”€ STORES â†’ pre/post test scores
  â”œâ”€â”€ STORES â†’ learning summaries
  â””â”€â”€ TRACKS â†’ session phase transitions

Scenario Configuration Engine
  â”œâ”€â”€ PROVIDES CONFIG TO â†’ Prompt System
  â”œâ”€â”€ PROVIDES MCQs TO â†’ Assessment Module
  â””â”€â”€ DEFINES â†’ misconceptions, subskills, levels

Prompt System
  â”œâ”€â”€ LOADS TEMPLATE FROM â†’ /app/prompts/
  â”œâ”€â”€ FILLS VARIABLES FROM â†’ Scenario Configuration
  â””â”€â”€ SENDS PROMPT TO â†’ AI Student Engine

AI Student Engine
  â”œâ”€â”€ CALLS â†’ OpenAI API (external)
  â”œâ”€â”€ RECEIVES PROMPT FROM â†’ Prompt System
  â”œâ”€â”€ PROVIDES RESPONSES TO â†’ User Interface
  â””â”€â”€ LOGS MESSAGES TO â†’ Logging System

Assessment Module
  â”œâ”€â”€ USES â†’ AI Student Engine (for tests)
  â”œâ”€â”€ GRADES â†’ MCQ responses
  â”œâ”€â”€ SUMMARIZES â†’ learning outcomes
  â””â”€â”€ CALCULATES â†’ improvement metrics

Logging & Analytics
  â”œâ”€â”€ WRITES TO â†’ /logs/runs/ (file system)
  â”œâ”€â”€ RECORDS â†’ all conversation turns
  â””â”€â”€ GENERATES â†’ session summaries
```

---

## Technology Stack Diagram

```
TECHNOLOGY LAYERS:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           PRESENTATION LAYER                â”‚
â”‚  Streamlit Web Application (Python 3.x)     â”‚
â”‚  - Chat UI, Forms, Cards, Metrics           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           APPLICATION LAYER                 â”‚
â”‚  main_streamlit.py (Orchestrator)           â”‚
â”‚  - Session management                       â”‚
â”‚  - Phase transitions                        â”‚
â”‚  - Message handling                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           SERVICE LAYER                     â”‚
â”‚  assessment.py    â”‚  prompt_loader.py       â”‚
â”‚  - Pre/Post tests â”‚  - Template loading     â”‚
â”‚  - MCQ grading    â”‚  - Variable substitutionâ”‚
â”‚  - Summarization  â”‚                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           DATA LAYER                        â”‚
â”‚  io.py (File Operations)                    â”‚
â”‚  - YAML reading (scenarios)                 â”‚
â”‚  - JSONL writing (transcripts)              â”‚
â”‚  - JSON writing (summaries)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           EXTERNAL SERVICES                 â”‚
â”‚  OpenAI API                                 â”‚
â”‚  - Chat Completions endpoint                â”‚
â”‚  - Models: GPT-4, GPT-4o-mini               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Session Flow State Diagram

```
STATE MACHINE:

[SETUP]
  â”‚
  â”‚ user clicks "Start Session"
  â†“
[PRE_TEST_RUNNING]
  â”‚
  â”‚ AI student answers MCQs
  â†“
[PRE_TEST_REVIEW]
  â”‚
  â”œâ”€â”€ user clicks question card â†’ [TEACHING]
  â”‚                                    â”‚
  â”‚                                    â”‚ conversation loop
  â”‚                                    â”‚ user clicks "Done"
  â”‚                                    â†“
  â”‚                              [SUMMARIZING]
  â”‚                                    â”‚
  â”‚                                    â”‚ generates learning summary
  â”‚                                    â†“
  â”‚â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â”‚ user clicks "End & Post-Test"
  â†“
[POST_TEST_RUNNING]
  â”‚
  â”‚ AI student answers with learning context
  â†“
[RESULTS]
  â”‚
  â”‚ user clicks "Reset Session"
  â†“
[SETUP]
```

---

## File Structure Diagram

```
PROJECT STRUCTURE:

ai_tutee_edtech_project/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main_streamlit.py          â† Entry Point
â”‚   â”‚
â”‚   â”œâ”€â”€ prompts/
â”‚   â”‚   â””â”€â”€ system_ai_student.md   â† AI Persona Template
â”‚   â”‚
â”‚   â”œâ”€â”€ scenarios/
â”‚   â”‚   â”œâ”€â”€ data_types.yaml        â† Scenario 1
â”‚   â”‚   â”œâ”€â”€ type_to_chart.yaml     â† Scenario 2
â”‚   â”‚   â”œâ”€â”€ chart_to_task.yaml     â† Scenario 3
â”‚   â”‚   â””â”€â”€ data_preparation.yaml  â† Scenario 4
â”‚   â”‚
â”‚   â””â”€â”€ util/
â”‚       â”œâ”€â”€ assessment.py          â† Test & Grading Logic
â”‚       â”œâ”€â”€ prompt_loader.py       â† Template Engine
â”‚       â””â”€â”€ io.py                  â† File Operations
â”‚
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ runs/                      â† Session Outputs
â”‚       â”œâ”€â”€ *.jsonl                â† Transcripts
â”‚       â””â”€â”€ *_summary.json         â† Summaries
â”‚
â”œâ”€â”€ requirements.txt               â† Dependencies
â””â”€â”€ README.md                      â† Documentation
```

---

## Key Interactions Summary

| From | To | Interaction Type | Data Exchanged |
|------|-----|------------------|----------------|
| User | UI Layer | Input | Scenario selection, chat messages |
| UI Layer | Session State | Read/Write | Phase, messages, scores |
| Session State | Scenario Engine | Read | Current scenario config |
| Scenario Engine | Prompt System | Provides | Misconceptions, subskills, level config |
| Prompt System | AI Engine | Sends | Complete system prompt |
| AI Engine | OpenAI API | HTTP POST | Chat completion request |
| OpenAI API | AI Engine | HTTP Response | Generated text/JSON |
| AI Engine | Assessment | Provides | Test responses |
| Assessment | UI Layer | Returns | Scores, summaries |
| AI Engine | Logging | Writes | Message records |
| Logging | File System | Writes | JSONL transcripts, JSON summaries |

---

## Diagram Generation Prompts

Use these prompts with diagram-generating LLMs:

### For a High-Level Architecture Diagram:
> "Create a system architecture diagram showing: User Interface (Streamlit), Session Manager, Scenario Engine (YAML configs), Assessment Module, AI Student Engine, Prompt System, Logging System, and OpenAI API as an external service. Show data flow arrows between components."

### For a Sequence Diagram:
> "Create a sequence diagram for an AI Tutee teaching session: User selects scenario â†’ System loads config â†’ Pre-test administered via OpenAI â†’ Teacher reviews results â†’ Teacher selects question â†’ Teaching conversation loop with OpenAI â†’ Learning summarized â†’ Post-test via OpenAI â†’ Results displayed â†’ Session logged."

### For a Component Diagram:
> "Create a component diagram with these modules: main_streamlit.py (orchestrator), assessment.py (testing), prompt_loader.py (templates), io.py (file ops), YAML scenario files, MD prompt templates, and external OpenAI API. Show dependencies between components."

### For a Data Flow Diagram:
> "Create a DFD showing: User input â†’ Session State â†’ Scenario Config â†’ System Prompt â†’ OpenAI API â†’ AI Response â†’ Assessment Grading â†’ Learning Summary â†’ Log Files. Include data stores for YAML configs and JSONL logs."
